<<echo=False>>=
import os
import pandas as pd
import scipy.stats  as stats
import matplotlib.pyplot as plt
import sys
import numpy as np
sys.path.append(os.getcwd())

from utils import *

FILM_TROPES_JSON_BZ2_FILE = '../datasets/scraper/cache/20190501/films_tropes_20190501.json.bz2'
FILM_EXTENDED_DATASET_TABLE_BZ2_FILE = '../datasets/extended_dataset.csv.bz2'
FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE = '../datasets/extended_dataset.json.bz2'
USE_HDF = True
SCRAPER_LOG_FILE = '../logs/scrape_tvtropes_20190501_20190512_191015.log'
MAPPER_LOG_FILE = '../logs/map_films_20190526_164459.log'
EVALUATOR_BUILDER_LOG_FILE = '../logs/build_evaluator_20190624_223230.log'
TOP_VALUES = 14
EVERYTHING_BUT_TROPES = ['Id','NameTvTropes', 'NameIMDB', 'Rating', 'Votes', 'Year']
EVALUATOR_HYPER_PARAMETERS_LOG_FILE = '../logs/build_evaluator_hyperparameters_20190622_203043.log'
RECOMMENDER_SUMMARY_LOG = '../logs/recommender_summary_20191201.log'


films_dictionary = read_compressed_json(FILM_TROPES_JSON_BZ2_FILE)
tropes_dictionary = reverse_dictionary(films_dictionary)


n_films = len(films_dictionary.keys())
n_tropes = len(tropes_dictionary.keys())
@

\errorcontextlines=3
\documentclass[APA,LATO1COL]{WileyNJD-v2}
\usepackage[utf8]{inputenc}
\usepackage{minted}

% -- begin: Added by Rubén
\usepackage{subcaption}
\captionsetup{compatibility=false}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{natbib}
\newcommand{\modification}[1]{{\color{black}#1}}

\usepackage{ragged2e}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}



\newcommand\BibTeX{{\rmfamily B\kern-.05em \textsc{i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\articletype{Article Type}%

\received{<day> <Month>, <year>}
\revised{<day> <Month>, <year>}
\accepted{<day> <Month>, <year>}

\begin{document}

\title{StarTroper, a film trope rating optimizer using \modification{Machine} Learning and Evolutionary Algorithms}

\author[1]{Rubén Héctor García-Ortega}

\author[2]{Pablo García-Sánchez}

\author[3]{Juan Julián Merelo-Guervós}



\address[1]{\orgdiv{Development Department}, \orgname{Badger Maps}, \orgaddress{\state{Granada}, \country{Spain}}}

\address[2]{\orgdiv{Department of Computer Science and Engineering}, \orgname{University of C\'adiz}, \orgaddress{\state{C\'adiz}, \country{Spain}}}

\address[3]{\orgdiv{Department of Computer Architecture and Technology}, \orgname{University of Granada}, \orgaddress{\country{Spain}}}

\corres{*Rubén Héctor García Ortega, Badger Maps. \email{ruben.hector.garcia.ortega@gmail.com}}

\presentaddress{Badger Maps. Granada, Spain.}

\abstract[Abstract]{
Designing a story is widely considered a crafty yet critical task that requires deep specific human knowledge
in order to reach a minimum quality and originality. This includes designing at a high
level different elements of the film; these high-level elements are called tropes when they become patterns.
The present paper proposes and evaluates a methodology to automatically synthesise sets of tropes
in a way that they maximize the potential rating of a film that conforms to them.
We use \modification{machine} learning to create a surrogate model that maps film ratings from tropes,
trained with the data extracted and processed from huge film databases in Internet,
and then we use a Genetic Algorithm that uses that surrogate model as evaluator
to optimize the combination of tropes in a film.
In order to evaluate the methodology, we analyse the nature of the tropes and their distributions in existing films,
the performance of the models and the quality of the sets of tropes synthesised.
The results of this proof of concept show that the methodology works and is able to
build sets of tropes that maximize the rating and that these sets are genuine.
The work has revealed that the methodology and tools developed
are directly suitable for assisting in the plots generation as an authoring tool
and, ultimately, for supporting the automatic generation of stories, for example, in massively populated videogames.}

\keywords{Content Generation; Tropes; Computational Narrative; Machine Learning; Genetic Algorithms}

\jnlcitation{\cname{%
\author{R.H. Garc\'ia-Ortega},
\author{P. Garc\'ia-S\'anchez},
\author{J.J. Merelo}} (\cyear{2019}),
\ctitle{StarTroper, a film trope rating optimizer using \modification{Machine} Learning and Evolutionary Algorithms}, \cjournal{}, \cvol{}.}

\maketitle


\section{Introduction}\label{sec1}

Crafting film scripts is quite challenging because of the plot complexities and the
\textit{multiplicative production function of entertainment} ~\citep{Hennig-Thurau2019},
that promulgate that the elements involved in the development of a media product need to work together
and a single failing one may provoke a disaster in cascade.
In fact, the concept of narrative itself can be seen as a complex adaptive system
where interactions between their elements or events make the story emerge \citep{Sack14Emergence}.
In order to tackle this complexity in the current research we are going to to describe the elements of the film
and how well they combine and interact; our candidates for both mechanisms are the
tropes that have been discovered in the films, and the massive human-evaluated ratings, respectively.

A trope is as a recurring narrative device or pattern, according to the definition by \citet{baldick2015oxford};
it could be a technique, a motif, an archetype or a \textit{clich\'e},
used by the script writers, producers and directors
to achieve specific effects that might vary from interest-increasing
to surprising through recall familiarity or entertaining,
in their creative works, such as books, films, comics or videogames.
Some tropes are broadly adopted and academically studied such as
the \textit{Three-act Structure} formulated by \citet{field1982screenplay},
the \textit{hero's journey} studied by \citet{vogler2007writer},
the \textit{McGuffin} popularized by Hitchcock, according to \citet{truffaut1985hitchcock}, and
the \textit{Chekhov's Gun} formulated by the Russian writer with the eponymous
name, according to \citet{bitsilli1983chekhov};
however, there are thousands of not-so-widely used tropes as well, discovered and
catalogued everyday by professionals and enthusiastic of the storytelling;
their study is organic, dynamic and extensive, according to~\citet{garcia2018overview}.
In fact, their emergence, self-organization and pattern formation are difficult to model,
implying they can be approached as a complex system  \citep{juarrero2000dynamics}.
In general, we might say that the set of tropes defines the overall narrative
architecture over which the narrative layer is eventually set. Tropes do not
define plot univocally, but constrain it in a number of ways. Thus, we can roughly
characterize a film by using the set of tropes that we can find in it.

Along this paper, we are going to use the analogy of the \textit{Film DNA} for
describing its set of tropes (and genres; hereafter when we say tropes we will
also include genres, since they are a type of meta-tropes); we will define it as
the set of tropes that are found in a film
and define things such as its structure, characters, events, mood, settings and narration.
As tropes are \textit{living concepts}, whose number grow as they are discovered as common
patterns in other stories, the \textit{Film DNA} is, by definition, incomplete and evolving,
yet it is still interesting to define stories, categorize them
and model them from a mathematical perspective.
The challenge of our research is to build original synthetic Film DNAs
based on a huge corpus of film-tropes, and through computational intelligence,
in a way that they have an intrinsic potential quality when reflected together in a film.
\modification{Our technique to build synthetic Film DNAs can achieve \textit{combinational creativity} \citep{boden2004creative}
because the generation of new Film DNAs is performed through the combination of old and familiar ideas.}

At the same time, we need to be able to associate a measure of quality to the \textit{Film DNA}
and it needs to summarize many factors, at last, perceived and evaluated by humans.
Luckily we have access to databases with films' information
that includes the genres of the films and their human evaluated ratings,
provided by the community of fans.
If we are able to construct a knowledge base of films, including the Film DNAs that we mentioned previously, the genres and the rating,
in a huge \textit{extended dataset}, we would be able to process them in order to make suggestions of tropes
that optimize the predicted rating;
however, even though intuitively
the \textit{Film DNA} is a profound way to describe a story
from many different perspectives, following the analogy of the DNA,
there are epigenetic factors that could deeply affect the performance of the story as well.
This method does not guarantee the quality of a film,
as a film that develops from a \textit{Film DNA} may implement them in infinite ways with very different
results in terms of quality;
however, it can be used as an indicator of the \textit{potential of the story} or the
most probable implementation based on the universe of currently analyzed films.
A synthetic film DNA might also find points in the film landscape that have not
been explored so far, making new blockbusters emerge in an arena that is
increasingly dominated by data analytics.

The main objective of this paper is to demonstrate how computational intelligence can be used to generate and improve
sets of tropes that maximize the potential rating of the films that conforms them,
in the context of authoring tools and Content Generation.
Our approach extracts <%=n_films%> \textit{Film DNAs} that contain in sum <%=n_tropes%> different tropes
from external Data Sources
and maps it to a database of film ratings and genres, dealing with disambiguation heuristics
in order to build what we have called the \textit{extended dataset}.
However, submitting a set of tropes to the box office is impossible, which is
why we use \modification{Machine} Learning \citep{lecun2015deep}
to create surrogate models that are able
to infer the rating from any combination of tropes.
We perform different analysis in order to determine the quality
of the predictions and the parameters that could affect them.
Later on, a Genetic Algorithm (GA) \citep{whitley1994genetic} and their operators are defined in a way
that the trope combinations, formerly Film DNAs, are evolved relying in the surrogate model to
maximize the rating.

The remaining of this work is organized as follows:
in the Section~\ref{sec:state_of_the_art} we explore the current state of the art
in plot generation based on tropes, in the Section~\ref{sec:methodology} we deepen the methodology presented above,
in the Section~\ref{sec:experimental_setup} we describe the experiments carried out to evaluate the methodology
and discuss the results,
and in the Section~\ref{sec:conclusions} we summarize the outcomes and future work.

\section{State of the art} \label{sec:state_of_the_art}

Film-makers, researchers and software developers are using known narrative patterns to build compelling stories,
whether through an authoring tool or an Automatic Content Generator.
Some of these patterns are being studied for decades,
for example, the Propp's formalism \citep{propp2010morphology}, first edited in 1928
and based on seven different roles, every one with a list of actions that can take over the course of a story,
in a fixed sequence of 31 functions.
Propp's formalisms are currently used by the computer scientists to build systems
that generates instances of Russian folk tales~\citep{gervs:OASIcs:2013:4156}
or stories and discourses with characters/places/objects relating to Iwate prefecture in Japan
through micro/macro story techniques~\citep{6505320}.
Other example of a popular narrative pattern widely used today is the \textit{hero's journey},
proposed by~\citet{campbell2008hero}, first edited in 1949,
and later on, reviewed from the perspective of the film industry by~\citet{vogler2007writer}.
It divides the story in 3 acts (\textit{departure}, \textit{initiation} and \textit{return}),
with 17 non-mandatory stages that are the universal scaffolding of ancient myths as well as modern day adventures.
The \textit{hero's journey} has been used for interactive storytelling in video games~\citep{delmas2007bringing},
and massive backstories generation~\citep{GarciaOrtega16MADE2}.

The Propp's formalism and the \textit{hero's journey}, as well as many other widely used narrative patterns, are included in the definition of trope.
According to~\citet{mellina2011trope},
"a trope is a unit of literary currency, recurring in works over time and gaining meaning
through audience recognition of its connotations and associations".
Their work, is one of the inspiration sources of our current research, as they use the tropes from
TV Tropes, a wiki that collects and documents descriptions and examples of tropes,
together with other features from IMDb, an online database of information related to multimedia content,
including the rating.
They use the \textit{Jaccard coefficient} to measure the
similarity between sets of tropes and discovered that
this similarity moderately predicts external measures of film acclaim.
As in their study, in our research we will use the two data sources, TV Tropes and IMDb,
and we will use the Jaccard coefficient as trope similarity as well.
However, in spite of having a similar base, their focus is the community detection
and ours is to use the dataset to predict the rating of Film DNAs.

Other authors are using the tropes from TV Tropes in their research as well.
In the work of \citet{Thompson18NarrativeEvents}, a system of agents relies on tropes to obtain a consistent narrative,
to describe the social norms that model the world in which they live.
The authors, as in this work, use tropes available on TV Tropes as a base
and translate them into logic statements that express duty or obligation,
using TropICAL language, which are the input for a logic programming solver;
however, they do not use all the tropes,
but a small set chosen by hand, which means that the range of resulting stories
is going to be limited.
\citep{guarneri2017ghost} built an authoring tool that proposes the use of tropes according to a narration structure,
and takes the objects from the card Game 'Once upon a time'.
They selected 94 elements out of 176 present in the Periodic Table of Tropes,
a subset of the most famous tropes from TV Tropes.

Nevertheless, it is very complicated to evaluate the content generated by an automatic generator,
not only because of its non-deterministic behaviour that makes it difficult to predict its outputs,
but also because of the subjective, diverse and stochastic nature of the audience,
as stated by \citet{TogeliusCap4Evaluating}.
To evaluate a generator one can use directly the opinion of the designer,
or indirectly from the audience, for example, using surveys, as in the work by \citet{guarneri2017ghost}.
In our research, we are also interested in a smart use of the tropes within a story but,
unlike \citet{guarneri2017ghost},
we are relying in Artificial Intelligence techniques
by simulating and estimating the quality of the content via some metrics,
while benefiting from the wisdom of collective opinions \citep{surowiecki2005wisdom} in IMDb for the rating.
In their work, \citet{hsu2014predicting} used different prediction models for the film rating
from features of the films in IMDb and, according to their evaluations, the Neural Network gave the best results,
in comparison with  linear combinations and multiple linear regressions.
In our current research, we will apply the same idea of using a Neural Network as predictor,
but fed from the film's tropes instead of from the film's features such as the director, actors or writers.
In fact, through the use of user-generated data, it is possible to obtain a large corpus
of examples to be used in computational narrative, as in the work of \citet{Guzdial15Crowdsourcing}.
Moreover, it is possible to extract information about review
sites, according to \citet{BoPang08OpinionMining}, such as MetaCritic or IMDb,
to be the input of a model like the one we propose in this article.

Some authors as \citet{bui2010evolving} have addressed the problem of the optimization of stories
by applying evolutionary computation. In their work, they developed a regular grammar model with causal relationships
and they evolve it, demonstrating that evolutionary computation can potentially contribute
significantly to story generations.
Our previous work also relies in Evolutionary Computation as a mechanism to optimize stories:
in \citep{GarciaOrtega15MADE}, we proposed the MADE framework:
a parameterizable multi-agent system that allowed
the generation of backstories in massive environments.
A GA was used to optimize the parameters of the system, for instance the simulation time,
the size of the world and the parameters of the behaviour of the agents,
with respect to the appearance of different archetypes, such as the \textit{hero} or the \textit{villain}.
These archetypes are defined by the possible actions that an agent may perform:
for example, the archetype Villain appears when an agent fights against another for food.
However, this form of evaluation was difficult to justify in order to measure the quality of the generated stories,
since it was based on an objective decision:
just the number of different character archetypes that emerge during the run of the world.
Also, the list of possible actions for the agents was very limited.
That is the reason that, later on, we proposed a more advanced model,
with more complex agents and the possibility of extracting knowledge from a logical reasoner \citep{GarciaOrtega16MADE2}.
On this occasion we used as quality metric the appearance of the \textit{hero's journey},
and the different archetypes that compose it.
In order to do this, logical reasoning was applied based on the predicates produced by the different events
that emerged in the system. However, as in previous work, the mere appearance of the \textit{hero's journey} does not fully serve
as a measure of the interest of the stories generated by the system.

That is why in this work we propose the combination of the previous ideas:
We will make use of the tropes as a way to model stories, in our case, films,
and we will optimize sets of tropes (our Film DNA) through evolutionary computation.
We will use a Neural Network as surrogate model of a \modification{GA} as well,
and we will train it with tropes from TV Tropes and features from IMDb, including the rating
in order to predict the rating.


\section{Methodology} \label{sec:methodology}


Our methodology is divided in four main steps, explained below and described
in the Figure~\ref{fig:main_workflow_extended}:

<<echo=False>>=
workflow = f'''
digraph {{
    splines=polyline
    rankdir=LR
    ranksep=0.25;
    margin=0;
    nodesep=0.3;
    graph [ resolution=128, fontsize=30];

    node [margin=0 fontcolor=black fontsize=10 width=1];
    tvtropes[label="TV Tropes\nwebsite\n\ntvtropes.org\n " type="database"];
    scrape_tropes[label="Step 1:\nScrape tropes\n\nPython+\nrequests+\nlxml+\nbz2 (blocksize 900k)+\ndisk cache\n~11.900 pages" type="process"];
    dataset[label="Dataset\n\nfilms->tropes\n({n_films}->{n_tropes})\n " type="data"];
    imdb[label="IMDb\ndatasets:\nimdb.com/\ninterfaces/" type="database"];
    map_rating[label="Step 2:\nDisambiguate\nfilms\n\nPython+\nHeuristics+\nbz2" type="process"];
    extended_dataset[label="Extended\nDataset\n\nFilm DNA+genres->\nrating" type="data"];
    build_evaluator[label="Step 3:\nBuild\nSurrogate\nModel\n\npandas+\nsklearn (MLPRegressor)\n " type="process"];
    evaluator[label="Surrogate model\n\nFilm DNA+genres->\nExpected\nRating\n\nMulti-layer\nPerceptron" type="tool"];
    user[label="User's\nconstraints\nfor the\nSynthetic\nFilm DNA" type="data"];
    dna_builder[label="Step 4:\nGenetic Algorithm\n\ninspyred+\ncachetools" type="process"];
    trope_sequence[label="Optimal\nSynthetic\nFilm DNA" type="data"];

    tvtropes -> scrape_tropes[minlen=0];
    scrape_tropes -> dataset[minlen=1];
    dataset -> map_rating;
    imdb -> map_rating[minlen=0];
    map_rating -> extended_dataset;
    extended_dataset -> build_evaluator;
    build_evaluator -> evaluator;
    evaluator -> dna_builder;
    user -> dna_builder[minlen=0];
    dna_builder -> trope_sequence;
}}'''

draw_graphviz(workflow, "main_workflow_extended.pdf")
@

\begin{itemize}
    \item[Step 1] Extract/scrape the tropes for every film
    and codify them as \textit{Film DNAs}.
    As we will explain, our dataset will have limitations derived
    from the fact that is fed from
    the community, finding that popular films are broadly described in terms of tropes
    and unpopular films poorly described or directly missing.
    We will analyze this variability and how it could affect the performance of the prediction model.
    \item[Step 2] Extract ratings and genres from an external Film Database finding the unequivocal film names
    and cull the original TV Tropes dataset.
    This \textit{extended dataset}  will show limitations as well
    based on the original one
    and the automatic matching based on different heuristics.
    As we will see, a trope that is widely used does not need to be linked to good ratings,
    tropes that are present in bad films can become good in different combinations and vice-versa.
    \item[Step 3] Build and train a surrogate model to predict the rating from a \textit{Film DNA}.
    We will follow different \textit{rules of thumb}
    to achieve a moderately good solution that serves our purposes.
    A multi-layer perceptron will handle the unknown relations between tropes and their combinations.
    \item[Step 4] Optimize the Film DNA with respect to ratings by
    building a \modification{GA} with specific operators
    that relies in the surrogate model previously built.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{figures/main_workflow_extended.pdf}
\caption[Methodology]{Methodology to generate constrained optimal Film DNAs using \modification{GAs} with Neural
Networks as surrogate models, fed from TV Tropes and IMDb.\label{fig:main_workflow_extended}}
\end{figure}



\subsection{Step 1: Extraction of tropes} \label{sec:methodology_scraper}


<<echo=False>>=
tropes_summary_dictionary = {}
for key in films_dictionary:
    tropes_summary_dictionary[key] = {'tropes':len(films_dictionary[key])}

tropes_summary_dataframe = pd.DataFrame(tropes_summary_dictionary).transpose()

films_summary_dictionary = {}
for key in tropes_dictionary:
    films_summary_dictionary[key] = {'films':len(tropes_dictionary[key])}

films_summary_dataframe = pd.DataFrame(films_summary_dictionary).transpose()
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = tropes_summary_dataframe.plot.hist(log=True, color='green', figsize=(5, 5.2), zorder=2, rwidth=0.5)
plot.set_xlabel("Films by number of tropes")
@
        \caption{}
        \label{fig:histogram_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(tropes_summary_dataframe.sort_values('tropes',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_tropes_films}
    \end{subfigure}

    \begin{subfigure}[b]{0.15\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.describe([.1,.2,.3,.4,.5,.6,.7,.8,.9]), fixed_width=True))%>
        \caption{}
        \label{fig:descriptive_analysis_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
plot = films_summary_dataframe.plot.hist(log=True, color='#1E77B4', figsize=(5, 5.2), zorder=2, rwidth=0.5)
plot.set_xlabel("Tropes by number of films")
@
        \caption{}
        \label{fig:histogram_films}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        <%=print(get_table_for_dataframe(films_summary_dataframe.sort_values('films',ascending=False).head(TOP_VALUES), fixed_width=True))%>
        \caption{}
        \label{fig:top_films_tropes}
    \end{subfigure}
    \caption{
    \textbf{(a)} Descriptive analysis of the Tropes by appearance in films.
    \textbf{(b)} Histogram of number of tropes by film (with logarithmic $y$ axis).
    \textbf{(c)} Top films by number of tropes.
    \textbf{(d)} Descriptive analysis of the tropes by number of films in which they appear.
    \textbf{(e)} Histogram of number of films by tropes (with logarithmic $y$ axis).
    \textbf{(f)} Top tropes by number of films.}
    \label{fig:films_analysis}
\end{figure}

We are going to use tropes as described in a live wiki called \citet{tvtropes_2}, that is
collecting thousand of descriptions and examples of tropes from 2014 until now.
As the data is fed by a community
of users, we could find the bias that popular films are better described and analysed in terms of the tropes
than older or independent films, and that popular tropes are more recognised than very specific ones.
Which means that, during the automatic generation of Film DNAs, tropes could be under/overrepresented,
and that positive and negative estimation errors are possible.
The semantic network of knowledge behind TV Tropes is huge and complex; it massively links hierarchies of tropes
to their usage in creations for digital entertainment. The data, however, is only available through its web interface,
which is why, in order to make it usable by the scientific community, \citet{maltekiesel_2} extracted all
their data to a database so-called \textit{DBTropes.org}.
As the base of the research on automatic trope generation, we begun with a dataset based in the
latest version of DBTropes, called PicTropes~\citep{garcia2018overview} that included 5,925 films with 18,270 tropes.
However, the last version of DBTropes is from 2016, and the community of users of TV Tropes has tripled the size
of the database since then; in other words, we are not using it because it is outdated.
If we work with the latest data from TV Tropes our machine learning algorithms
would benefit from having much entries and hence, provide better results. That is why our first step is to
extract the data directly from TV Tropes while making it available to the public
and the researchers,
in the context of the Open Science.

Our scraper, which is also released as free software in the Python ecosystem under
the {\tt tropescraper} name, and is also available from GitHub
(\url{https://github.com/raiben/tropescraper}),
extracts all the categories from the main
categories page and, for every one of them, it extracts all the film identifiers assigned to it.
Finally, for every film page, it extracts all the trope identifiers, building
a dictionary of films and tropes.
Trope identifiers are written in \textit{CamelCase} format and may include the year to avoid ambiguity.
Some technical details are listed in Figure~\ref{fig:main_workflow_extended}.

The resulting dataset includes <%=n_films%> \textit{Film DNAs} and <%=n_tropes%> tropes.
In both cases, the number of tropes by film and film by tropes follow long tail distributions, where
a large number of occurrences are far from the "head" or central part of the distribution,
as shown in Figure~\ref{fig:films_analysis}.
60\% of the films have 40 or less tropes
but there are films with more than 800 tropes.
On the other hand, most tropes appear in 6 films, but there are tropes with more than 3000 occurrences in films.
These figures will have to be taken into account when we analyse the expected quality of the evaluator and
the distribution of evaluation errors, and during the experimental setup, in order to make decisions
according to the observed bias.

It is part of the current research to analyze the expected effect of this distribution
in the results of applying our methodology.
The first conclusion is that we have many more samples with a small number of tropes than with many;
however, at this step we do not have enough information to elucidate if this situation is explained
by the fact that it is user-generated data and the popularity
defines how well described are the films in terms of tropes, but we can assume that, in general, that is the case.
Furthermore, we cannot make out yet a relationship between the number of tropes
in a film and its rating, but according to the Figure \ref{fig:films_analysis}c,
the films with the highest number of tropes are mostly last-generation superhero movies
are popular and broadly acclaimed by the critic,
and that suggests a positive correlation between rating and number of tropes.
However, as the next section complements the tropes with additional information,
such as the rating, the genres or the number of votes,
we will be in a position where we can find correlations
that help us explain the possible results of the experiments in a better way.

\subsection{Step 2: Disambiguation of films to get the rating} \label{sec:methodology_mapper}

TV Tropes is a huge yet very specific database of tropes but it does not include a rating
or links to an external database that we could use as a rating source;
on the other hand, IMDb offers their database for non-commercial use and they provide
datasets with lots of interesting features,
including the rating and the number of votes.
Our research just needs a way identify a movie in TV Tropes with another in IMDb.

IMDb Datasets are a compendium of information that IMDb offers for personal and
non-commercial use \citep{imdbDatasets}.
Our current research will make use of these datasets to extend the film information from TV Tropes, in particular,
\textit{titles}, which contains metadata from the films such as the title, the year, the genres
and the duration, and \textit{ratings}, which contains the rating and the number of votes.

Items in IMDb that don't relate to films are excluded (tvEpisode, tvSeries, tvSpecial, tvShort, videoGame,
tvMiniSeries, titleType) because they are not in our TV Tropes scraped dataset and they would only increase
ambiguity as more films might match the same name.
In order to be able to map the film names,
films names are normalized in both cases, TV Tropes and IMDb, converting \textit{CamelCase} format to \textit{Title case}, removing
non-alphanumerical values and extra blanks, splitting name and year when required, and converting to lowercase,
considering the original title and the English title.
Normalized names in TV Tropes and IMDb are matched, ideally \{1->1\}, but
in practice, especially when the year is not declared, we tend to find a big list of candidates
for every single film in TV Tropes.
In order to reduce ambiguity, if the year is present in TV Tropes's identifier,
we reduce the search to the specific year in IMDb, and, in any case,
we select the candidate with the highest number of votes.
This heuristic relies in the fact that both data sources (tropes and votes)
are generated by different communities of users, enthusiasts in both cases,
so if there is a film in TV Tropes and there are many films with the same name in IMDb,
it will probably be the one with highest popularity, that is reflected in the number of votes.


<<echo=False>>=
extended_dataframe = read_dataframe(FILM_EXTENDED_DATASET_TABLE_BZ2_FILE, USE_HDF)
trope_names = [key for key in extended_dataframe.keys() if key not in EVERYTHING_BUT_TROPES and '[GENRE]' not in key]
extended_dataframe['Number of tropes'] = sum(getattr(extended_dataframe,key) for key in trope_names)

coefficient_rating_votes, p_value_rating_votes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Votes'])
coefficient_rating_ntropes, p_value_rating_ntropes = stats.pearsonr(extended_dataframe['Rating'], extended_dataframe['Number of tropes'])
coefficient_votes_ntropes, p_value_votes_ntropes = stats.pearsonr(extended_dataframe['Votes'], extended_dataframe['Number of tropes'])
coefficient_year_ntropes, p_value_year_ntropes = stats.pearsonr(extended_dataframe['Year'], extended_dataframe['Number of tropes'])
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Number of tropes', y='Rating', color='#1E77B4', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Number of tropes', 'Rating', 'black')
#plt.grid(b=True, which='major', color='#666666', linestyle='-')
#plt.minorticks_on()
#plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_rating_ntropes%> \\ p-value & <%=p_value_rating_ntropes%> \end{tabularx}}
        \label{fig:rating_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Votes', y='Number of tropes', color='#1E77B4', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Votes', 'Number of tropes', 'black')
#plt.grid(b=True, which='major', color='#666666', linestyle='-')
#plt.minorticks_on()
#plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_votes_ntropes%> \\ p-value & <%=p_value_votes_ntropes%> \end{tabularx}}
        \label{fig:votes_ntropes}
    \end{subfigure}
    \hfill
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
extended_dataframe.plot.scatter(x='Year', y='Number of tropes', color='#1E77B4', label='Films', figsize=(4, 4))
plot_regression(extended_dataframe, 'Year', 'Number of tropes', 'black')
#plt.grid(b=True, which='major', color='#666666', linestyle='-')
#plt.minorticks_on()
#plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)
@
        \caption{\begin{tabularx}{\textwidth}{lX} coefficient: & <%=coefficient_year_ntropes%> \\ p-value & <%=p_value_year_ntropes%> \end{tabularx}}
        \label{fig:year_ntropes}
    \end{subfigure}
    \hfill
    \caption{Scatter plot (with fitting) to show the relation between: \textbf{(a)} Rating vs. \# of tropes
    \textbf{(b)} Votes vs. \# of tropes \textbf{(c)} Year vs. \# of tropes}
    \label{fig:extended_dataset_scatterplots}
\end{figure}



First, we need to explore if there are significant biases in the original set, since it's generated by the user,
and we have included features such as the rating, the number of votes and the year.
For this reason, the Figure \ref{fig:extended_dataset_scatterplots} shows the scatter plots
of the relations between the number of tropes and
the rating, the rating and the number of tropes, and the number of votes and the year in the new \textit{extended dataset},
consisting in <%=extended_dataframe.shape[0]%> films linked to <%=len(trope_names)%> tropes.

Figure \ref{fig:extended_dataset_scatterplots}b shows that there is a
significant positive correlation between the number of tropes and the rating;
in other words, a
higher number of tropes is related to higher rating, and vice-versa.
It is important to remark that the films up to $\sim53$ tropes (the average) or less, show the widest range,
$\{\sim 1.3-\sim9.5\}$, for the rating,
whereas films with more than 600 tropes have a rating in the range $\{6-\sim9.5\}$.
If we want to test the methodology, choosing a fixed number of tropes lower or
equal to $53,$ wuch as the median the median (30), which what we are doing,
would allow us to find an optimal solution in a wider range while keeping the
search space small. At the same time, a number of tropes in this range
has an average rating in the range $\{6-6.5\}$, according to the regression function,
which is good enough. No search algorithm will guarantee that a synthetic set of
tropes will reach that average, but at least we know that we will be able to
generate movies with good or excellent rating if we work with that specific number
of tropes.

There seems to be also a significant positive correlation between the number of
tropes and the votes (popularity) as well, which
might explain the long tail distributions in the previous step, Figure~\ref{fig:films_analysis};
in other words,
the more popular the film is, the better it is described by the community and the more tropes are found.
This outcome implies that, if the experimental setup fixes the number of tropes for the synthetic Film DNA,
it does not necessary imply fixing the complexity but fixing the detail,
that probably ends up determining the evaluation error.
Finally, there is a significant positive correlation between the number of tropes and the year,
according to the scatter plot, the year of the film \modification{limits} the maximum number of tropes that describe the films
in our dataset.

These three findings will help us unravel the limitations of a surrogate model fed from the \textit{extended dataset}
and used to predict the rating from a set of tropes. The analysis points out that choosing a small Film DNA
will lead to films not very well described,
with potential to have a rating in a wider range of values, but also
a bigger chance to be evaluated with errors than films with more tropes.

<<echo=False>>=
checker = TropesSimilarityChecker()
checker.load_extended_dataset_json(FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE)
mean_genres = statistics.mean([len([trope for trope in film['tropes'] if '[GENRE]' in trope]) for film in checker.films])
@

As we stated before, the occurrence matrix of tropes in films is very sparse
because most of the films in TV Tropes are described with just a few tropes
whereas the minority have a huge number, and the reason is that it is user-generated data.
However, we confirm that the films have a mean of <%= human_readable(mean_genres) %> genres,
so we consider that the estimation will benefit from considering the genres as features, especially
when the films are poorly described.
That is the reason why, although genres and tropes are different concepts,
we will consider both in our \textit{extended dataset} during the next steps,
as they both serve to describe the story at different layers.

\subsection{Step 3: The surrogate model}  \label{sec:methodology_surrogate_model}

<<echo=False>>=
input, output = get_experiment_execution_information(EVALUATOR_BUILDER_LOG_FILE)
layers = output[output['parameter']=='Layer sizes']['value'][0].split(', ')
@

We will use the \textit{extended dataset},
to build a trope-to-rating approximator
that can then be used as a surrogate model by the GA.

As explained in the Section~\ref{sec:methodology_mapper}, the tropes and genres have
a relationship of belonging with the film, and hence, the proposed way to represent the input of the evaluator
is a list of boolean whose indexes map to the list of possible tropes.

The output is a continuous numeric value that represents the rating of the film, theoretically from 0 to 10.
As the average number of tropes by film is 53, and the possible number of tropes is <%=n_tropes%>,
99\% of the cells in the matrix will have a value of 0. In other words, it will be very sparse,
with a small representation of the tropes in the catalogued films.
The \modification{machine} learning technique most suitable in this context needs to expose feature-extraction capabilities
in order to deal with unknown and unbalanced relations between tropes to achieve a specific rating.
Although there are different candidates that could perform properly under these circumstances,
in our current research we choose a neural network \citep{schmidhuber2015deep}.

The goal of our research in this initial stage is to evaluate a methodology,
so tuning the surrogate model is carried out as far as it suits the needs in terms of quality of the estimations,
with reasonable performance and a low error rate.
There are many decisions that can define the quality of the model;
some of them will be made based on the state of the art
and, for others, we will have to make hyper-parameters search.
In general, although there are many rules of thumb to build acceptable neural networks,
results may differ drastically depending on the nature of the problem and
it is recommended to do a hyper-parameters evaluation.
We selected the multi-layer perceptron (MLP), the most widespread neural network architecture,
because it has been proven to be able to approximate any function that we require,
the so called \textit{Universal Approximation Theorem} \citep{hornik1989multilayer}.
In order to choose hyper-parameters that get along with the nature of our problem
we did a preliminary search with all the combinations in a domain of possible values
for the activation (ReLu or tanh),
the number of hidden layers (1 or 2),
the number of neurons in each layer (162 or 883/29) according to the geometric pyramid rule proposed by \citet{masters1993practical},
the learning rate (constant or adaptive)
and the solver (Adam or SGD).
We applied 3-fold cross\modification{-}validation and obtained the average and the standard deviation.

<<echo=False>>=
iterations_evaluator = extract_iterations_from_log(log_file_name=EVALUATOR_BUILDER_LOG_FILE)
gridsearch_dataframe = extract_grid_parameters_from_log_and_results(log_file_name=EVALUATOR_HYPER_PARAMETERS_LOG_FILE)
@
\begin{table}
    \centering
<%=print(get_table_for_dataframe(gridsearch_dataframe))%>
    \caption{Hyper-parameters evaluation using 3-fold cross validation, sorted by validation score}
    \label{fig:gridsearch_results}
\end{table}


\modification{The \textit{validation score} is the mean of the scores of the model trained with 90\% of the data (training data),
against 10\% of the data not used for training (validating data) for every k-fold.}
The results in Table~\ref{fig:gridsearch_results} show that
a MLP with the structure <%='/'.join(layers)%>, using ReLu activation,
constant learning rate and SGD solver provides the best validation score.
After training the MLP using the selected hyper-parameters, the \textit{extended dataset} as input
and the rating as output, until it does not improve more than the tolerance for 10 consecutive runs,
the evaluation converged to a training mean squared error (MSE) of <%=human_readable(iterations_evaluator['loss'].iloc[-1])%>
and the a validation MSE of <%=human_readable(iterations_evaluator['validation'].iloc[-1])%>,
that implies that the model is a good predictor as both values are quite similar, and that it is not overfitting.
The root mean squared error (RMSE), that is a metric in rating units, has the value of
<%=human_readable(math.sqrt(iterations_evaluator['validation'].iloc[-1]))%>
for new predictions, that implies that in some cases the evaluations might be above 10.

<<echo=False>>=
info_file = u'../datasets/evaluator_tests.json.bz2'
test = EvaluatorTests()
test.init_from_file(info_file)
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes = pd.DataFrame(test.test_results['errors_by_tropes'])
errors_by_tropes.plot.hexbin(x='Number of Tropes', y='Error', gridsize=25, cmap="Blues", bins="log", figsize=(5, 5.2))
plot.set_xlabel("Number of Tropes")
@
        \caption{}
        \label{fig:errors_by_tropes}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes_stats = pd.DataFrame(test.test_results['errors_by_tropes_stats'])
errors_by_tropes_stats.plot.scatter(x='Number of Tropes', y='Average', figsize=(5, 5.2))
@
        \caption{}
        \label{fig:average_errors}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
errors_by_tropes_stats = pd.DataFrame(test.test_results['errors_by_tropes_stats'])
errors_by_tropes_stats.plot.scatter(x='Number of Tropes', y='Standard Deviation', figsize=(5, 5.2))
@
        \caption{}
        \label{fig:average_std}
    \end{subfigure}
    \caption{\textbf{(a)} Hexagonal binning plot of the evaluation errors for all the films (color in logarithmic scale).
    \textbf{(b)} Average absolute value of the error by number of tropes.
    \textbf{(c)} Standard deviation of the absolute error value by number of tropes}
    \label{fig:errors}
\end{figure}

The multi-layer perceptron will be used as a surrogate model in a specific experiment in further sections,
so, in order to anticipate the results, we need to analyze how well it predicts
the ratings grouped by the number of tropes, since we will have to limit them
during the optimization process.
We calculate the error for every sample of our \textit{extended dataset};
Figure~\ref{fig:errors}(a) presents it as a hexagonal binning plot. This plot
shows that most of the cases are positioned close to an error of value 0 (high density).
There are under and over-estimations, especially in the range of tropes with more occurrences \{0-100\},
but the it's more likely that rating is under-estimated;
in general, the surrogate model will tend to under-estimate for a small number of tropes,
and estimation will be increasingly accurate when the number of tropes is increased.
This is consistent with the fact that movies with more
tropes are more popular and they are probably better described.

Figure~\ref{fig:errors}(b) and (c) shows the average absolute error and absolute standard deviation by the number of tropes.
Again, we can observe that a small number of tropes (less than 200) implies higher errors and higher deviations than
a bigger number of tropes. That is explained because we are working with user-generated content
and less popular films are poorly described in terms of number of tropes, hence their rating is harder to predict.
At the same time, it points out that the number of tropes is a good predictor,
and, if the films were described with an equal level of detail, it would imply more tropes and less errors.

<<echo=False>>=
mean_1_genre = statistics.mean([item['Estimated rating'] for item in test.test_results['evaluations_1_genre']])
#Out[9]: 5.511383624560505
stdev_1_genre = statistics.stdev([item['Estimated rating'] for item in test.test_results['evaluations_1_genre']])
#Out[10]: 0.010139587722208073
genres = [item['Genre'].replace('[GENRE]','') for item in test.test_results['evaluations_1_genre']]
@

Analysing the weight of the genre on the rating is also a good outcome in order to know if there are rating biases
regarding the genre. We did the evaluation of all the Film DNAs that contain only one genre out of the <%=len(genres)%>,
getting an average value of <%=human_readable(mean_1_genre)%>(+-<%=human_readable(stdev_1_genre)%>), so, in principle,
choosing a single genre or another does not seem to limit the rating. Intuitively, that tells us
that all the genres have good and bad films in the same proportion and our synthetic Film DNA will not suffer
from strong biases due the genre. However, it is the combination of genres what
might boost the rating, and this is precisely where the optimization process
comes into play.


\subsection{The Genetic Algorithm} \label{sec:methodology_ga}

At this step we already have a set of candidate tropes for the Film DNA and the
surrogate model trained and evaluated in the previous step,
so our goal is to use a system that allows us to generate a Film DNA that optimizes the rating.
We determine to use a GA
because it is a mechanism that allows us to explore the domain of Film DNAs,
in other words, of combinations of tropes, getting high-quality solutions and dealing with global optimization problems.

Our chromosome will be the Film DNA, that is, a set of tropes and genres.
In practice it will be encoded as an array of different indices on a dictionary of the total set of tropes available
without value repetitions, given that, \textit{a priori}, the order of the tropes is irrelevant
(according to their nature, they may refer to specific moments, but also to narrative structures and general settings)
and also, our rating evaluator does not consider weights or multiple occurrences of the trope in the movie
(both, training data and model do not consider multiple occurrences of a trope in a single Film, in other words,
the trope appears or does not appear). In practice, it will be a list of 30 tropes
and genres with no repetitions.

The mutation operator changes a trope of the Film DNA by another that is randomly chosen
from the set of tropes not included of the Film DNA, which avoids repetition of
tropes/genres, and allows an exploration of new tropes.
The crossover operator will make a superset of tropes from the parents' Film DNAs
and randomly selects two subsets for the offspring.
This way, the offspring will have Film DNAs whose tropes are exclusively from their parents, allowing
the exploitation of the data.

As we explained above, we have to rely on a surrogate model for the evaluation of Film DNAs, and our approach uses
a neural network trained with existing movies, the one that has been presented
in the previous section. The fitness of our GA will be the result of evaluating
the set of tropes and genres the neural network, that is, their predicted rating.

According to our tests, the GA has proven to converge towards optimal solutions efficiently,
given the simplicity of the operators based on set algebra and fitness calculation.
It is important to remark that the GA performs an optimization step that is
essential for the research as far as it leads us to our goal,
that is to prove that the methodology works, and hence,
an exploratory analysis of the parameters of the GA is reasonable in this context,
as we will see in the next section, \ref{sec:experimental_setup}.

\section{Experimental setup} \label{sec:experimental_setup}

The evaluation of the methodology is performed unequivocally through its testing and a further
analysis of the results. In this research, we want to know if the methodology
can be used to synthesise an optimal Trope DNA for a standard movie in terms of potential rating.

Our experiment \modification{used} a fixed size Film DNA of 30 tropes, which is the median of tropes per film
in the original dataset and therefore, a candidate for a 'standard' film.
As benefits, a Film DNA of size 30 is easier to interpret in natural language than one of size 200.
Furthermore, as we saw in the Section~\ref{sec:methodology_mapper}, this number of tropes is a good candidate
because it can lead to a wide range of ratings, good and bad,
and the GA will have a better chance to evolve solution from bad cases;
however, as we saw in the Section~\ref{sec:methodology_surrogate_model}, a Film of size 30 implies
more chances to under or over-evaluate, and the rating will be less accurate.
We also use\modification{d} a fixed length chromosome because at this stage we were only
interested in a proof of concept and, otherwise, the results could be harder to explain and interpret.

\modification{Our decision to use a Genetic Algorithm implies the existence of a population, manipulated by selection,
recombination, and mutation operators \citep{eiben2011parameter}, that is why we need to focus on
the parameters Population size ($\mu$), Mutation rate ($p_m$) and Crossover rate ($p_c$).
However,}  the most suitable parameters to solve a specific problem require an extensive calibration, which is outside
of the scope of this paper. \modification{Nevertheless}, to choose the set of parameters that we \modification{used} in all the experiments
in this paper, we \modification{made} a preliminary selection \modification{from combinations of values of Population size ($\mu$),
Mutation rate ($p_m$) and Crossover rate ($p_c$) chosen through rules of the thumb,
so we could test the methodology with different parameter combinations.
The candidates values for $\mu$ were:
\begin{itemize}
\item 50, the \textit{de facto} value suggested by \citet{de1990analysis};
\item 100, the default value in the library \texttt{inspyred} \citep{LibraryR37:online}; and
\item 200, the next element in the exponential series $\mu = 50*2^{n}$, for $n\in \left \{ 0, 1, 2 \right \}$.
\end{itemize}
The candidates for $p_m$ circle the popular idea of having 1 mutation per locus,
also proposed formally by \citet{harvey2001artificial}, in our case $1\div size_{dna} = 0.0\widehat{3}$.
We included as candidates the previous value divided by 2 ($0.5\div size_{dna} = 0.01\widehat{6}$)
and multiplied by 2 ($2\div size_{dna} = 0.0\widehat{6}$).
The candidates for $p_c$ included 0.75, 0.5 and 0.25, so we explored different convergence alternatives,
from high to slow, that would perform better or worse depending on the nature and complexity of the search space.
In summary, we explored how the GA performs changing the values of 3 parameters
and we chose 3 candidates for each one based on widely adopted rules of the thumb,
that leaded to 27 possible combinations to test, each one run 30 times, that was translated to 810 runs.
As stated previously, we are aware that our research may have limitations due the lack of more fine parameter tuning,
especially because the parameter values can largely influence the performance of the algorithm
\citep{eiben2011parameter}, but our first selection aimed
to be a first working approach with an appropriate number of combinations and execution time,
avoiding the exponential effect of having more variables and values.

\begin{figure}
    \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
parameters_df = pd.read_csv(RECOMMENDER_SUMMARY_LOG,
                 names=['a','b','c','Pm','Pc','μ','d','e','fitness','f']+list(range(0,30)),
                 sep=',', dtype={'Pm':np.float64, 'Cp':np.float64, 'Sp':np.float64, 'fitness':np.float64},
                 decimal=".")
parameters_df = parameters_df.round(4)
parameters_df.boxplot(column='fitness', by=['Pm','Pc','μ'], return_type='axes', figsize=(14,4), rot=90)
plt.title("")
plt.suptitle("")
plt.ylabel('fitness')
@
    \caption{\modification{Fitness distribution of the best individuals for the 30 runs
            for every parameter combination of the Genetic Algorithm ($p_m$, $p_c$, $\mu$).
            The best combination,
            $p_m$ = $0.5\div size_{dna}$, $p_m$ = 0.25, and $\mu$ = 200,
            presents an average fitness of <%=human_readable(9.602308)%> (+-<%=human_readable(0.541747)%>).
            }}
    \label{fig:ga_parameters}
\end{figure}

<<echo=False>>=
ga_setup_dictionary = {
    "Parameter": ["Population size ($\mu$)","Mutation probability ($p_m$)", "Crossover probability ($p_c$)", "Minimum number of evaluations", "Stop criterion", "Number of runs"],
    "Value": ["$200$", "$0.5\div size_{dna} = 0.01\widehat{6}$", 0.25, 3000, "No improvements in the population in the last 10000 evaluations",30]
}

ga_setup_dataframe = pd.DataFrame.from_dict(ga_setup_dictionary)
@

\begin{table}
    \modification{
    \centering
<%=print(get_table_for_dataframe(ga_setup_dataframe))%>
    \caption{Selected parameters used to set up the Genetic Algorithm}
    \label{fig:ga_setup}
    }
\end{table}

We executed}  30 times every possible combination of Population size \modification{($\mu$)} = \{50, 100, 200\},
Mutation probability \modification{($p_m$)} = \{\(2\div size_{dna}\), \(1\div size_{dna}\), \(0.5\div size_{dna}\)\}
and Crossover Probability \modification{($p_c$)} = \{0.25, 0.5, 0.75\} in order to achieve minimal statistical significance.
\modification{Each run out of the 30 performed a minimum of 3,000 evaluations and stopped when
the best generation was not improved during 10,000 evaluations, both values set after empirical trials and errors.
\modification{The results in Figure \ref{fig:ga_parameters} show} that
the combination \modification{$\mu$} = 200, \modification{$p_m$} = \modification{$0.5\div size_{dna}$}
and \modification{$p_m$} = \modification{0.25},
obtains better results on average. Therefore we will use this configuration,
summarized in Table \ref{fig:ga_setup} to improve the readability of the paper \ref{fig:ga_setup},
to perform the rest of the experiments.}

As previously stated, a set of tropes is not a full-fledged film, so we need to have an idea of what a movie
with that set of tropes would look like. This is why, in order to interpret the results,
we \modification{used} metrics of similarity between finite sample sets.
The first one is a metric called \textit{Jaccard coefficient} \modification{($C_{j}$), a metric that can successfully
used for keywords similarity \citep{niwattanakul2013using}},
and is defined as the cardinality of the intersection of Film DNAs \modification{($T_A$ and $T_B$)}
divided by the cardinality of the union of the Film DNAs. \modification{A value of 0 implies that there are no tropes in common,
and a value of 1 implies that both Film DNAs are equal.}

\modification{
\[ C_{j}(T_A,T_B) = \frac{| T_A  \cap T_B |}{|T_A  \cup T_B|} \]
}
The Jaccard coefficient is interesting to measure not only what two sets have in common,
but also, how different their sizes are, penalizing big differences in length of the sets.
However, as we \modification{were going to compare} our synthetic Film DNA of a fixed size of 30,
popular films with hundred of tropes \modification{were going to} be penalized just because they
are too broadly described in comparison;
in order to address this \modification{issue}, we also \modification{used} a coefficient
based only in the Common Elements \modification{method ($C_{c}$)}
and is defined as the intersection between the sets \modification{divided by the length of the synthetic Film DNA ($L(T_S)$)}.
\modification{
\[C_{c}(T_S,T_B) = \frac{|T_S  \cap T_B|}{L(T_S)} \]
}

\modification{The last stage of the experiment consisted in analysing a sample synthetic Film DNA with a high potential rating
obtained through our methodology, in order to
intuitively illustrate how the set of tropes defines the nature of a film.}

\section{Results}

The goal of this specific experiment is to check if the surrogate model can be used successfully for
optimizing a Film DNA by its potential rating.
According to the experimental setup in Section~\ref{sec:experimental_setup},
the selected length of the Film DNA is 30, as it has been analysed to be a good candidate.
The Film DNA \modification{includes} tropes and genres, as discussed previously in Section~\ref{sec:methodology_mapper},
because the estimation \modification{can} benefit from considering the genres as features, specially when the number of tropes is small.

\modification{As we explained before, we ran the GA 30 times with different random seeds, obtaining 30 solutions (the best synthetic Film DNA of each run).
Figure \ref{fig:ratings_30_runs} shows a boxplot of the potential rating of the solutions
with an average of <%=human_readable(9.60231)%> (+-<%=human_readable(0.541747)%>),
that confirms that the GA can evolve solutions to very high potential ratings.

In order to confirm the originality of the solutions, we calculate\modification{d} the Jaccard and Common
Elements similarity coefficients
among all the different pairs of synthetic Film DNAs for the 30 solutions (435 pairs of solutions in total),
what we call \textit{intra-synthetic} coefficients in Figure \ref{fig:jaccard_1},
and among all the different pairs of synthetic and existing film DNAs in the \textit{extended dataset}
(322980 pairs of solutions in total),
what we call \textit{inter-corpus} coefficients in Figure \ref{fig:common_1}.}

<<echo=False>>=
ratings, jaccard, common = get_solutions_analysis(FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE,
                                                  RECOMMENDER_SUMMARY_LOG)
@

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
ax = ratings.plot.box(return_type='axes', figsize=(3,4))
ax.grid(axis='y')
@
        \caption{}
        \label{fig:ratings_30_runs}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
jaccard.boxplot()
plt.yscale('log')
@
        \caption{}
        \label{fig:jaccard_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
<<width="\\linewidth",echo=False, results='hidden'>>=
common.boxplot()
plt.yscale('log')
@
        \caption{}
        \label{fig:common_1}
    \end{subfigure}
    %------second row---
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \normalsize
<%=print(get_table_for_dataframe(ratings.describe()))%>
        \caption{}
        \label{fig:ratings_describe}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
<%=print(get_table_for_dataframe(jaccard.describe()))%>
        \caption{}
        \label{fig:jaccard_describe}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
<%=print(get_table_for_dataframe(common.describe()))%>
        \caption{}
        \label{fig:common_describe}
    \end{subfigure}

    \caption{\modification{\textbf{(a, d)} Rating distribution and descriptive analysis of the 30 obtained synthetic Film DNAs.
    \textbf{(b, e)} Distribution and descriptive analysis of the \textit{Jaccard coefficients} ($C_{j}$) for all pairs of combinations in
                 the synthetic Film DNA set (intra-synthetic) and compared to the \textit{extended dataset} (inter-corpus), with a logarithmic $y$-axis.
    \textbf{(c, f)} Distribution and descriptive analysis of the \textit{Common Elements coefficients} ($C_{c}$) for all pairs of combinations in
                 the synthetic Film DNA set (intra-synthetic) and compared to the \textit{extended dataset} (inter-corpus), with a logarithmic $y$-axis.
    }}
    \label{fig:coefficients}
\end{figure}

\modification{
As we can observe, the synthetic films generated through our methodology and our current configurations are more similar
between themselves than among the existing films. According to Table \ref{fig:jaccard_describe},
average intra-synthetic $C_j$ is
<%=human_readable(0.0586199)%> (+-<%=human_readable(0.0282036)%>)
and average inter-corpus $C_j$ is <%=human_readable(0.0117854)%> (+-<%=human_readable(0.0144976)%>),
and according to Table \ref{fig:common_describe},
average intra-synthetic $C_c$ is
<%=human_readable(0.109425)%> (+-<%=human_readable(0.0497876)%>)
and average inter-corpus $C_c$ is <%=human_readable(0.0252263)%> (+-<%=human_readable(0.0281026)%>).
Both metrics confirm that the results are original in terms of existing films.
In both metrics, we can see that averages and percentiles are in the same order of magnitude,
with slightly higher values in $C_c$.
The values of $C_c$ are intuitively easy to transform to shared tropes simply by multiplying by 30;
that tells us that, on average the existing films share <%=human_readable(0.0252263*30)%> tropes or genres with an synthetic film DNA.
In some cases it is 0 tropes or genres shared and, in most cases, 1, and, very rarely, up to 7.
However, the synthetic Film DNAs tend to have more elements in common among themselves,
on average <%=human_readable(0.109425*30)%>,
in some cases 0, in most cases 3 or 4 and, very rarely, up to 8. That indicates that there exist some trope combinations
found by our algorithm that are correlated with higher ratings.

Furthermore, as part of the analysis, we interpreted one of the Film DNA obtained,}
that has a potential rating of <%=human_readable(10.313031174658876)%> and the following tropes.\\


\begin{align*}
& DNA_{Film} = \{ActionHeroBabysitter, DeathByFlashback, DisneyVillainDeath, DuelToTheDeath,\\
& \qquad\qquad\qquad EarlyBirdCameo, FightingFromTheInside, HandsOffParenting, Homage, \\
& \qquad\qquad\qquad ImNotAfraidOfYou, JumpCut, MouthingTheProfanity, NoSympathy, OminousFog, \\
& \qquad\qquad\qquad OneHeadTaller, PoorMansSubstitute, PragmaticAdaptation, \\
& \qquad\qquad\qquad RichIdiotWithNoDayJob, SomeoneToRememberHimBy, SpitefulSpit, TalkingHeads,\\
& \qquad\qquad\qquad TitledAfterTheSong, WeaponOfXSlaying, \\
& \qquad\qquad\qquad [GENRE]Animation, [GENRE]Documentary, [GENRE]Drama, [GENRE]History, \\
& \qquad\qquad\qquad [GENRE]Mystery, [GENRE]Romance, [GENRE]War, [GENRE]Western\}
\end{align*}


<<echo=False>>=
checker = TropesSimilarityChecker()
checker.load_extended_dataset_json(FILM_EXTENDED_DATASET_DICTIONARY_BZ2_FILE)
tropes_list = ['ActionHeroBabysitter', 'DeathByFlashback', 'DisneyVillainDeath', 'DuelToTheDeath',
               'EarlyBirdCameo', 'FightingFromTheInside', 'HandsOffParenting', 'Homage', 'ImNotAfraidOfYou',
               'JumpCut', 'MouthingTheProfanity', 'NoSympathy', 'OminousFog', 'OneHeadTaller',
               'PoorMansSubstitute', 'PragmaticAdaptation', 'RichIdiotWithNoDayJob', 'SomeoneToRememberHimBy',
               'SpitefulSpit', 'TalkingHeads', 'TitledAfterTheSong', 'WeaponOfXSlaying', '[GENRE]Animation',
               '[GENRE]Documentary', '[GENRE]Drama', '[GENRE]History', '[GENRE]Mystery', '[GENRE]Romance',
               '[GENRE]War', '[GENRE]Western']
top_overlap, top_jaccard, top_common = checker.get_top_films_by_simmilarity(tropes_list, 6)
@

The synthetic Film DNA belongs to a multi-genre film (historical documentary romantic animation drama,
set during a war, that includes western and mystery settings, not comedy).
However, according to one of the tropes, even if it is historical/documentary,
it is completely an adaptation from the author with clear differences with the overall known story
for pragmatic reasons (\textit{PragmaticAdaptation}).
The following explanations are derived from the current definitions in TV Tropes and,
in any case, they should be interpreted as just one example out the vast number of possible implementations,
specially considering that tropes and genres evolve through time and are continually discovered,
adapting to the new films and cultural trends.

In the synthetic Film DNA, there are tropes that define the main characters:
one of them is a rude action character that, in this case, is compelled to
have a position of responsibility for children (\textit{ActionHeroBabysitter}),
that could be related to the existence of irresponsible parents (\textit{HandsOffParenting}).
There is a female character, as required by the trope \textit{SomeoneToRememberHimBy},
and there is a couple, according to \textit{OneHeadTaller}, where one character is clearly taller or shorter than the other.
One of the characters is rich and has a lot of free time (\textit{RichIdiotWithNoDayJob}), and,
according to the trope \textit{DisneyVillainDeath} there is a villain as well.
There are tropes that define the conflicts and how they will develop:
characters resisting an external influence acting on them (\textit{FightingFromTheInside}),
a villain is finally fought by a protagonist when this character is \textit{not afraid} anymore (\textit{ImNotAfraidOfYou}),
there is a duel (\textit{DuelToTheDeath}),
a special weapon is used (\textit{WeaponOfXSlaying}),
the villain falls off (\textit{DisneyVillainDeath}),
one of the protagonists heroically dies as well, and in the end it is discovered that
the female protagonist is or was pregnant, as required by the trope \textit{SomeoneToRememberHimBy}.
There are also tropes that define the setting:
there is fog (\textit{OminousFog}) as part of the mystery genre
and a clear \textit{'Homage'} to a classic or well known artwork in the same genres is present.
According to the narrative perspective,
the story includes a flashback that points to the death of the main character/s (\textit{DeathByFlashback}),
uses 'Jump Cuts' as an editing technique (\textit{JumpCut}) and make a character appear before his/her introduction (\textit{EarlyBirdCameo}).
Some tropes also define very specific sequences: in some cases the film includes spits (\textit{SpitefulSpit})
and the characters swear, although it cannot be heard by the audience (\textit{MouthingTheProfanity}).
There is also a scene where terrible things have happened to the main characters but no-one acts as it is really important (\textit{NoSympathy}),
and there are scenes with no action, just long conversations where people do not move from their place (\textit{TalkingHeads}).
According to the meta-tropes present,
the action character use to be an \textit{action hero} in previous movies,
and, in contrast, in the current one the character deals with unfamiliar problems and domestic situations.
Also, one of the actors is relatively unknown but looks alike another well known one (\textit{PoorMansSubstitute})
and the film's name is a reference to an existing song (\textit{TitledAfterTheSong}).
As we mentioned above, the film DNA has 8 genres (out of 27), that can be seen as a high number for a film,
but there are examples of TV series
with a high number of genres as well, like Dr. Who or Stranger Things, with 5 genres each.
\\

As part of the analysis, we need to confirm the originality of the Film DNA, so
we perform\modification{ed} a similarity analysis against the whole \textit{extended dataset}
and we \modification{found} that the coefficients are small:
According to the Jaccard metric, the most similar films have a value of <%=top_jaccard[0].jaccard %>, in other words,
they are  <%=human_readable_percent(top_jaccard[0].jaccard*100) %>\% similar at maximum.
According to the Common Elements metric, the most similar films have a value of <%=human_readable(top_common[0].common_tropes_count/30)%>
that is higher than the Jaccard coefficient because it is not penalizing the difference of length of the Film DNAs,
in other words, the maximum number of common tropes/genres between the whole \textit{extended dataset} and the
\modification{synthetic} Film DNA is <%=top_common[0].common_tropes_count %>. There are only 6 films in the whole dataset
with that similarity,
presenting all of them the genre Drama and the tropes PragmaticAdaptation, most of them the genre Romance,
and to a lesser extent the genre Mystery and the trope Homage.


Our \modification{synthetic} Film DNA not only has a very high potential rating but also is very different to the rest of the films
in our \textit{extended dataset}, that is positive because it proves that the GA is exploring
combinations that are far from those already realized in real life.

\modification{More obtained examples of high rated synthetic Film DNAs and random ones with lower rates are available in
the project's repository \footnote{\url{https://github.com/raiben/made_recommender/blob/master/datasets/film_dna_examples_20191201.csv}}.}


\section{Conclusions} \label{sec:conclusions}

Tropes in films and their relations can be seen as an adaptive, un-predictable
and dynamic complex system, with an emerging behavior that would be the overall
effect provoked on the public, that can eventually be measured by the rating
given to them. Optimizing such a complex system can be approached from a number
of different ways, which is why the goal of this research is to evaluate a
methodology that automatically synthesises sets of tropes
in a way that they maximize the potential rating of a film that uses them.
In order to do so, we have first extracted all the tropes from the TV Tropes sites and the genres and ratings from IMDb,
merging them in a single \textit{extended dataset}
and dealing with technical challenges as the devise of heuristics to disambiguate film names
and the consequent analysis of distribution of the data, that affects the further steps.
The scraped we have used has been released in the Python ecosystem as TropeScraper
\url{https://pypi.org/project/tropescraper/}. This will allow easy updating of
the tropes database for new experiments. Our research group also supports open
science, so the results of all experiments and the data used in them is published
in \url{https://github.com/raiben/made_recommender}.

We used the dataset to train a MLP that predicts the rating from a Film DNA from its set of tropes and genres,
and used it as the surrogate model in an evolutionary algorithm that tries to
find a set of tropes and genres that maximizes rating; this evolutionary algorithm
uses a fixed-length set of tropes and is able to find a set with a rating that is,
in fact, higher than the maximum rating found in the training set.

This first work on the subject establishes the methodology through a proof of
concept, and will also be used to establish a series of baseline measurements that
can eventually be used to compare with further developments of this methodology.
This proof of concept already proves that the GA that uses a MLP as surrogate
model to evaluate fitness of sets of tropes is able to find genuine Film DNAs
with a very high potential rating, and is able to deal with the intrinsic difficulties
of working with this \textit{complex system} of films, tropes, genres and ratings.

The best result, which includes several genres among the synthetic film DNA, shows
that, even if the individual contribution of every genre to rating is not too
important, combining several genres, even more so in an environment such as this
proof of concept where the number of tropes/genres in the chromosome is limited,
makes sense and boosts the rating, on average (9.7) and also in the best case
analyzed, which includes 8 genres.
The effect of using a different number of tropes, even unlimited, or limiting the genres or not including them at all,
is unknown and interesting as these changes could affect the expected rating and the applicability of the tropes.
This is left, however, as future work.
On the other hand, building a Film DNA might be the most direct application of the methodology,
and a good fit for a proof of concept but there are other applications that could have explained the
boundaries of the proposal and the limitations of the surrogate model, such as improving an existing film
or mixing films.

We also conclude that the tropes and the genres are good estimators of the quality
and that the neural network architecture chose, a MLP, works well as rating estimator, with a low error rate that decreases as the number of tropes grow.
However, a MLP is a black box, hard to optimize and to explain, so we wonder
how using other techniques could affect the rating and the possible applications;
this exploration is also left as future work.

Moreover, during this proof of concept we discovered limitations of the dataset that could compromise
the quality of the evaluator:
first of all, due the very nature of the user-generated data, the most popular and new films get a more detailed
description in terms of tropes, hence the films with less tropes are more prone to be evaluated with errors.
We decided to add the genres to mitigate this effect on unpopular films, but other techniques
to reduce the dimensionality \modification{such as Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA)},
\modification{could help us analysing the trope space}, making the dataset more dense and reducing the time
to train the surrogate model.
However, we decided not to use them because they would bring more complexity to the proof of concept.

A synthetic Film DNAs directly applicable to help on the
preparation and design of a film as we shown in the results section,
because every trope or genre has a different and specific translation according to TV Tropes:
in some cases, it affects to the structure, narration, characters, elements, actions, moments and/or places in a general or specific way.
in other cases, it affects to elements out of the story, like the casting, direction guidelines or production.
So essentially, some tropes and genres will be achievable and other will not, and this brings two new questions:
what to do if a trope needs to be avoided in a solution and what to do if a trope is required to complement the story.
Our assumption is that both questions can be tackled through the modification of the GA operators but that would
also benefit from having explicit dependencies between the tropes and the genres.
At the same time,  we want to remark that the final application of a Film DNA in the creation process,
working with the coherence among tropes and genres,
filling the gaps with elements in the story and narrating them in a compelling way
is still required, not part of the scope of the current research and,
as far as we know, mostly a human responsibility
that the final rating depends on.

All previous considerations may open a promising and encouraging research line,
hence we propose that further research should be undertaken in different areas. First, the GA could be improved by implementing constraints such as limiting the genres, dealing with tropes dependencies,
enforcing the appearance of tropes/genres, penalizing tropes/genres or promoting certain Film DNA similarity.
This way, further experiments could help us define the
problems that the methodology can address, for example,
allowing the generation without trope number restrictions,
improving an existing Film DNA in a way that some tropes are replaced but a minimum similarity is achieved,
mixing the tropes of two or more films in a way that the resulting film DNA
keeps the essence of them while maximizing the rating,
or even massively generating film DNAs that share common tropes, leading to
backstories that cross in time, and ultimately,
can help us populate the virtual world of a videogame.

\modification{As stated in the introduction, the presented technique can achieve
\textit{combinational creativity}, making use of old and familiar tropes to build new Film DNAs.
However, more complex creativity types such as the \textit{exploratory creativity} or the \textit{transformational creativity}
could only be tackled through the decomposition of tropes in concepts and the categorization of tropes,
requiring more elaborated techniques to extract information from natural language sentences found in TV Tropes.
According to \citet{boden2009creativity}, being able to extract these concepts from the tropes and
using them to build new tropes would help us to achieve higher levels of creativity
while making use of natural semantic constraints to avoid misusing resources on generating
invalid Film DNAs.}

The surrogate model could be improved as well in any number of different ways;
in order to simplify the arithmetic of tropes/genres in the set and explain hidden relations between them,
further researches could implement a technique
similar to word2vec,
assigning a vector in a multi-dimensional space to every trope/genre, in a way that
the ones that share common contexts in the corpus are located close in the space.
Also, the quality of the dataset could be improved by applying techniques to reduce the dimensionality,
such as \textit{principal components analysis} and \textit{feature selection},
improving the performance of the evaluator.
\modification{TV Tropes also offers information of sub and super tropes, allowing building
a directed graph of categories that includes cycles;
using this information during the creation of the model could enrich it, providing new tools to reduce noisy tropes
and filling in the gaps of the missing ones during the data wrangling phase.}

Finally, any further research focused in automating the translation of tropes to
coherent actions in a virtual world would be a decisive contribution to make the
results suitable for a content generator of stories.

\section*{Acknowledgements}
This work has been partially funded by projects DeepBio (TIN2017-85727-C4-2-P) and TEC2015-68752
and ``Ayuda del Programa de Fomento e Impulso de la actividad Investigadora de la Universidad de C\'adiz''.

\bibliography{report}
\end{document}
